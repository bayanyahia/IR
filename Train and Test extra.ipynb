{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define function to read text files and create DataFrame\n",
        "def folder_to_csv(folder_path):\n",
        "    data = {'title': [], 'txt': [], 'subject': []}\n",
        "\n",
        "    # Iterate through files in the folder\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.txt'):  # Process only text files\n",
        "                file_path = os.path.join(root, file)\n",
        "                subject = os.path.basename(root)  # Get folder name as subject\n",
        "                with open(file_path, 'r', encoding='latin-1') as f:  # Specify 'latin-1' encoding\n",
        "                    txt_content = f.read()\n",
        "                data['title'].append(file.split('.')[0])  # Get file name without extension\n",
        "                data['txt'].append(txt_content)\n",
        "                data['subject'].append(subject)\n",
        "\n",
        "    # Create DataFrame from collected data\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Specify folder paths\n",
        "folder1_path = '/content/Filter bubble'\n",
        "folder2_path = '/content/Ai explainability'\n",
        "\n",
        "# Convert folders to DataFrame\n",
        "df_folder1 = folder_to_csv(folder1_path)\n",
        "df_folder2 = folder_to_csv(folder2_path)\n",
        "\n",
        "# Concatenate DataFrames from both folders\n",
        "df_combined = pd.concat([df_folder1, df_folder2], ignore_index=True)\n",
        "\n",
        "# Save combined DataFrame to CSV\n",
        "df_combined.to_csv('combined_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "5xoM5VEcyBM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def find_word_in_files(folder_path, word):\n",
        "    files_with_word = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.txt'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                with open(file_path, 'r', encoding='latin-1') as f:  # Use 'latin-1' encoding\n",
        "                    text = f.read()\n",
        "                    if word in text:\n",
        "                        files_with_word.append(file_path)\n",
        "    return files_with_word\n",
        "\n",
        "# Specify folder path and word to search\n",
        "folder_path = '/content/Ai explainability'\n",
        "word_to_find = 'filter bubble'\n",
        "\n",
        "# Find occurrences of the word in text files within the folder\n",
        "matching_files = find_word_in_files(folder_path, word_to_find)\n",
        "\n",
        "# Print the list of files containing the word\n",
        "if matching_files:\n",
        "    print(f\"Occurrences of '{word_to_find}' found in the following files:\")\n",
        "    for file_path in matching_files:\n",
        "        print(file_path)\n",
        "else:\n",
        "    print(f\"No occurrences of '{word_to_find}' found in the folder.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4r0Abt4WCm7w",
        "outputId": "9e50fdc2-a130-489d-bad5-e2443d674422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No occurrences of 'filter bubble' found in the folder.\n"
          ]
        }
      ]
    }
  ]
}