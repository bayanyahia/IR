{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcqQlFo9CYQu",
        "outputId": "4e5313e5-e149-4303-9518-a76e095da2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 most frequent words:\n",
            "1. the : 2166\n",
            "2. of : 1796\n",
            "3. and : 1179\n",
            "4. a : 1054\n",
            "5. in : 955\n",
            "6. discrimin : 596\n",
            "7. to : 512\n",
            "8. for : 419\n",
            "9. is : 401\n",
            "10. by : 390\n",
            "11. on : 357\n",
            "12. data : 292\n",
            "13. as : 231\n",
            "14. from : 215\n",
            "15. econom : 213\n"
          ]
        }
      ],
      "source": [
        "def count_tf(file_name):\n",
        "    with open(file_name, 'r', encoding=\"latin-1\") as f:\n",
        "        # reading data of the file\n",
        "        read_data = f.read()\n",
        "        tokens = word_tokenize(read_data)\n",
        "        # Stemming the tokens\n",
        "        stemmed_tokens = [ps.stem(token) for token in tokens]\n",
        "        freq_dist = FreqDist(stemmed_tokens)\n",
        "        sorted_freq = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)  # Sort in descending order\n",
        "        return sorted_freq\n",
        "\n",
        "def print_top_15(token_frequencies):\n",
        "    print(\"Top 15 most frequent words:\")\n",
        "    for i, (token, frequency) in enumerate(token_frequencies[:15], start=1):\n",
        "        print(f\"{i}. {token} : {frequency}\")\n",
        "\n",
        "file_name = \"/content/cleaned_text.txt\"\n",
        "token_frequencies = count_tf(file_name)\n",
        "print_top_15(token_frequencies)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "# Initialize Porter Stemmer and get stop words\n",
        "ps = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def count_tf_without_stopwords(file_name):\n",
        "    with open(file_name, 'r', encoding=\"latin-1\") as f:\n",
        "        # reading data of the file\n",
        "        read_data = f.read()\n",
        "        tokens = word_tokenize(read_data)\n",
        "        # Filter out stop words and stem tokens\n",
        "        filtered_tokens = [ps.stem(token) for token in tokens if token.lower() not in stop_words]\n",
        "        freq_dist = FreqDist(filtered_tokens)\n",
        "        sorted_freq = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
        "        return sorted_freq\n",
        "\n",
        "def print_top_15(token_frequencies):\n",
        "    print(\"Top 15 most frequent words after removing stop words:\")\n",
        "    for i, (token, frequency) in enumerate(token_frequencies[:15], start=1):\n",
        "        print(f\"{i}. {token} : {frequency}\")\n",
        "\n",
        "file_name = \"/content/cleaned_text.txt\"\n",
        "token_frequencies = count_tf_without_stopwords(file_name)\n",
        "print_top_15(token_frequencies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_nnN1R7ETLE",
        "outputId": "1ae92ac1-9c71-479f-cf79-c34f043215e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 most frequent words after removing stop words:\n",
            "1. discrimin : 596\n",
            "2. data : 292\n",
            "3. econom : 213\n",
            "4. group : 204\n",
            "5. model : 181\n",
            "6. statist : 154\n",
            "7. studi : 149\n",
            "8. racial : 142\n",
            "9. journal : 140\n",
            "10. j : 138\n",
            "11. analysi : 135\n",
            "12. regress : 134\n",
            "13. use : 127\n",
            "14. market : 110\n",
            "15. employ : 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize Porter Stemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def count_tf_after_stemming(file_name):\n",
        "    with open(file_name, 'r', encoding=\"latin-1\") as f:\n",
        "        # reading data of the file\n",
        "        read_data = f.read()\n",
        "        tokens = word_tokenize(read_data)\n",
        "        # Stemming the tokens\n",
        "        stemmed_tokens = [ps.stem(token) for token in tokens]\n",
        "        freq_dist = FreqDist(stemmed_tokens)\n",
        "        sorted_freq = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
        "        return sorted_freq\n",
        "\n",
        "def print_top_15(token_frequencies):\n",
        "    print(\"Top 15 most frequent stemmed words:\")\n",
        "    for i, (token, frequency) in enumerate(token_frequencies[:15], start=1):\n",
        "        print(f\"{i}. {token} : {frequency}\")\n",
        "\n",
        "file_name = \"/content/cleaned_text.txt\"\n",
        "token_frequencies = count_tf_after_stemming(file_name)\n",
        "print_top_15(token_frequencies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMM5UUqvEeZd",
        "outputId": "0f8d17ae-279f-45c3-aae4-8bddc76986c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 most frequent stemmed words:\n",
            "1. the : 2166\n",
            "2. of : 1796\n",
            "3. and : 1179\n",
            "4. a : 1054\n",
            "5. in : 955\n",
            "6. discrimin : 596\n",
            "7. to : 512\n",
            "8. for : 419\n",
            "9. is : 401\n",
            "10. by : 390\n",
            "11. on : 357\n",
            "12. data : 292\n",
            "13. as : 231\n",
            "14. from : 215\n",
            "15. econom : 213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def count_tf_after_case_folding(file_name):\n",
        "    with open(file_name, 'r', encoding=\"latin-1\") as f:\n",
        "        # reading data of the file\n",
        "        read_data = f.read()\n",
        "        # Convert the text to lowercase\n",
        "        read_data = read_data.lower()\n",
        "        # Tokenize the text\n",
        "        tokens = word_tokenize(read_data)\n",
        "        freq_dist = FreqDist(tokens)\n",
        "        sorted_freq = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
        "        return sorted_freq\n",
        "\n",
        "def print_top_15(token_frequencies):\n",
        "    print(\"Top 15 most frequent words after case folding:\")\n",
        "    for i, (token, frequency) in enumerate(token_frequencies[:15], start=1):\n",
        "        print(f\"{i}. {token} : {frequency}\")\n",
        "\n",
        "file_name = \"/content/cleaned_text.txt\"\n",
        "token_frequencies = count_tf_after_case_folding(file_name)\n",
        "print_top_15(token_frequencies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uICmV7KBEu_g",
        "outputId": "8b02a4c0-cbbe-4477-cd87-7f519866aa46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 most frequent words after case folding:\n",
            "1. the : 2166\n",
            "2. of : 1796\n",
            "3. and : 1179\n",
            "4. a : 1054\n",
            "5. in : 955\n",
            "6. discrimination : 571\n",
            "7. to : 512\n",
            "8. for : 419\n",
            "9. is : 401\n",
            "10. by : 390\n",
            "11. on : 357\n",
            "12. data : 292\n",
            "13. as : 231\n",
            "14. from : 215\n",
            "15. that : 193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove punctuation marks except for hyphens and apostrophes (to preserve words like \"we'll\" or \"co-operation\")\n",
        "    cleaned_text = re.sub(r'[^\\w\\s\\'-]', '', text)\n",
        "    # Remove multiple spaces\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
        "    # Remove leading and trailing spaces\n",
        "    cleaned_text = cleaned_text.strip()\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to read the file, clean its contents, and save into a new file\n",
        "def clean_and_save_file(original_file, new_file):\n",
        "    with open(original_file, 'r', encoding=\"latin-1\") as f:\n",
        "        # Read the contents of the file\n",
        "        file_content = f.read()\n",
        "        # Clean the text using the clean_text function\n",
        "        cleaned_text = clean_text(file_content)\n",
        "    # Write the cleaned text into a new file\n",
        "    with open(new_file, 'w', encoding=\"latin-1\") as f:\n",
        "        f.write(cleaned_text)\n",
        "\n",
        "# Example usage:\n",
        "original_file_name = \"/content/A multidisciplinary survey on discrimination analysis.txt\"\n",
        "new_file_name = \"/content/cleaned_text.txt\"\n",
        "clean_and_save_file(original_file_name, new_file_name)\n",
        "print(f\"Cleaned text saved into '{new_file_name}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT2g-GvHFEAG",
        "outputId": "77bb2d71-634a-4e91-ff14-787513389d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text saved into '/content/cleaned_text.txt'\n"
          ]
        }
      ]
    }
  ]
}